{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "BA_과제5_3_BigramNewsGroup.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQBQdUur-Amm"
      },
      "source": [
        "## 20 Newsgroups data 준비\n",
        "\n",
        "http://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
        "\n",
        "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
        "<br><br>\n",
        "특징: \n",
        "* train set과 test set이 별도로 분리되어 있음.\n",
        "* categories 매개변수를 이용하여 20개의 topic 중에서 원하는 topic을 선택할 수 있음\n",
        "* remove를 이용하여 필요없는 부분을 삭제할 수 있음\n",
        "* 각 set 내에서 .data 는 text 내용을, .target은 숫자로 변경된 label(category)를 가져오는 데 사용됨\n",
        "\n",
        "따라서 20 newsgroups는 train_test_split을 이용하여 train set과 test set을 분리할 필요가 없음\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VLykXuF-Amr",
        "outputId": "1d98fe6b-926a-45e1-e903-2843b74b0b88"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train',\n",
        "#메일 내용에서 hint가 되는 부분을 삭제 - 순수하게 내용만으로\n",
        "                                      remove=('headers', 'footers', 'quotes'),\n",
        "                                      categories=categories)\n",
        "\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', \n",
        "                                     remove=('headers', 'footers', 'quotes'),\n",
        "                                     categories=categories)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IoRlM9S-Amu",
        "outputId": "7f62a159-3d00-4d54-bada-0879eb1d3907"
      },
      "source": [
        "print('train set size:', len(newsgroups_train.data))\n",
        "print('test set size:', len(newsgroups_test.data))\n",
        "print('selected categories:', newsgroups_train.target_names)\n",
        "print('train labels:', set(newsgroups_train.target))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set size: 2034\n",
            "test set size: 1353\n",
            "selected categories: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
            "train labels: {0, 1, 2, 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3ki5Bf9-Amv",
        "outputId": "dc690c33-b634-4c5e-9dc8-84f688cf16f7"
      },
      "source": [
        "print('##Train set text samples:', newsgroups_train.data[0])\n",
        "print('##Train set label smaples:', newsgroups_train.target[0])\n",
        "print('##Test set text samples:', newsgroups_test.data[0])\n",
        "print('##Test set label smaples:', newsgroups_test.target[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##Train set text samples: Hi,\n",
            "\n",
            "I've noticed that if you only save a model (with all your mapping planes\n",
            "positioned carefully) to a .3DS file that when you reload it after restarting\n",
            "3DS, they are given a default position and orientation.  But if you save\n",
            "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
            "know why this information is not stored in the .3DS file?  Nothing is\n",
            "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
            "I'd like to be able to read the texture rule information, does anyone have \n",
            "the format for the .PRJ file?\n",
            "\n",
            "Is the .CEL file format available from somewhere?\n",
            "\n",
            "Rych\n",
            "##Train set label smaples: 1\n",
            "##Test set text samples: TRry the SKywatch project in  Arizona.\n",
            "##Test set label smaples: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9f18Cb--Amv"
      },
      "source": [
        "newsgroups_train과 newsgroups_test로부터 .data와 .target을 이용하여 X_train, X_test, y_train, y_test을 추출하여 text document classification을 수행하시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5z4GdvC-Amw"
      },
      "source": [
        "X_train = newsgroups_train.data\n",
        "X_test = newsgroups_test.data\n",
        "y_train = newsgroups_train.target\n",
        "y_test = newsgroups_test.target"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwzGc14A-Amw",
        "outputId": "e1bdb911-8aff-4e03-bb9f-0e79e6282724"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "cachedStopWords = stopwords.words(\"english\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAiTNDMD-Amx",
        "outputId": "42fde69d-976d-4552-aa7e-f3374d454dda"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(token_pattern= \"[a-zA-Z']{3,}\", \n",
        "                        decode_error ='ignore', \n",
        "                        lowercase=True, \n",
        "                        stop_words = stopwords.words('english'), \n",
        "                        max_df=0.5,\n",
        "                        min_df=2).fit(X_train)\n",
        "X_train_tfidf = tfidf.transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "print(X_train_tfidf.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2034, 11483)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctpMEto6-Amy",
        "outputId": "97c0a028-a9a4-4424-d86a-6b02b9c34cd5"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression \n",
        "clf = LogisticRegression() #분류기 선언\n",
        "clf.fit(X_train_tfidf, y_train) # train data를 이용하여 분류기를 학습\n",
        "print('Train set score: {:.3f}'.format(clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
        "print('Test set score: {:.3f}'.format(clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set score: 0.966\n",
            "Test set score: 0.761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05Bn5M_q-Amz"
      },
      "source": [
        "### Bigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1FTe_is-Amz",
        "outputId": "a2ce2e6e-8ee0-4eec-eec5-781a8fd95162"
      },
      "source": [
        "tfidf = TfidfVectorizer(token_pattern= \"[a-zA-Z']{3,}\", \n",
        "                        decode_error ='ignore', \n",
        "                        lowercase=True, \n",
        "                        stop_words = stopwords.words('english'),\n",
        "                        ngram_range=(1, 2),\n",
        "                        max_df=0.5,\n",
        "                        min_df=2).fit(X_train)\n",
        "X_train_tfidf = tfidf.transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "print(X_train_tfidf.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2034, 26550)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-JV83AV-Am0",
        "outputId": "edb54cd3-9e36-4557-fe0f-24e5ce117bc7"
      },
      "source": [
        "bigram_features = [f for f in tfidf.get_feature_names() if len(f.split()) > 1]\n",
        "print(bigram_features[:10])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'cause can't\", \"'em better\", \"'expected errors'\", \"'karla' next\", \"'nodis' password\", \"'official doctrine\", \"'ok see\", \"'sci astro'\", \"'what's moonbase\", 'aas american']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQJdV-1c-Am0",
        "outputId": "4eb70db9-e5cd-4174-dd28-909ac28a8f75"
      },
      "source": [
        "clf = LogisticRegression() #분류기 선언\n",
        "clf.fit(X_train_tfidf, y_train) # train data를 이용하여 분류기를 학습\n",
        "print('Train set score: {:.3f}'.format(clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
        "print('Test set score: {:.3f}'.format(clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set score: 0.969\n",
            "Test set score: 0.756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwqtDXzM-Am1"
      },
      "source": [
        "### Trigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJMxtbB2-Am1",
        "outputId": "aed620e6-604a-4025-bdad-9fcc1f8e01bc"
      },
      "source": [
        "tfidf = TfidfVectorizer(token_pattern= \"[a-zA-Z']{3,}\", \n",
        "                        decode_error ='ignore', \n",
        "                        lowercase=True, \n",
        "                        stop_words = stopwords.words('english'),\n",
        "                        ngram_range=(1, 3),\n",
        "                        max_df=0.5,\n",
        "                        min_df=2).fit(X_train)\n",
        "X_train_tfidf = tfidf.transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "print(X_train_tfidf.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2034, 32943)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLVKhD37-Am2",
        "outputId": "519e752b-ffe8-4372-8775-6439bf66628e"
      },
      "source": [
        "trigram_features = [f for f in tfidf.get_feature_names() if len(f.split()) > 2]\n",
        "print(trigram_features[:10])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'em better shots\", \"'expected errors' basically\", \"'karla' next one\", \"'nodis' password also\", \"'official doctrine think\", \"'ok see warning\", \"'what's moonbase good\", 'aas american astronautical', 'ability means infallible', 'able accept donations']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W40D-Zhm-Am2",
        "outputId": "8588ddae-0f04-48b8-f03a-96c83d0b0726"
      },
      "source": [
        "clf = LogisticRegression() #분류기 선언\n",
        "clf.fit(X_train_tfidf, y_train) # train data를 이용하여 분류기를 학습\n",
        "print('Train set score: {:.3f}'.format(clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
        "print('Test set score: {:.3f}'.format(clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set score: 0.969\n",
            "Test set score: 0.758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvphifCD-Am2"
      },
      "source": [
        "### Ridge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wWC8O8X-Am3",
        "outputId": "342fb219-c248-403d-d09b-5a5cdee944c5"
      },
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "ridge_clf = RidgeClassifier() #릿지 분류기 선언\n",
        "ridge_clf.fit(X_train_tfidf, y_train) #학습\n",
        "print('Train set score: {:.3f}'.format(ridge_clf.score(X_train_tfidf, y_train)))\n",
        "print('Test set score: {:.3f}'.format(ridge_clf.score(X_test_tfidf, y_test)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set score: 0.976\n",
            "Test set score: 0.775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Unc_syCU-Am3"
      },
      "source": [
        "### Lasso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C01Hnsdv-Am3",
        "outputId": "e38728a0-3dd0-4be2-dfb7-743df9a94f21"
      },
      "source": [
        "import numpy as np\n",
        "lasso_clf = LogisticRegression(penalty='l1', solver='liblinear') # Lasso는 동일한 LogisticRegression을 사용하면서 매개변수로 지정\n",
        "lasso_clf.fit(X_train_tfidf, y_train) # train data로 학습\n",
        "print('Train set score: {:.3f}'.format(lasso_clf.score(X_train_tfidf, y_train)))\n",
        "print('Test set score: {:.3f}'.format(lasso_clf.score(X_test_tfidf, y_test)))\n",
        "print('Used features count: {}'.format(np.sum(lasso_clf.coef_ != 0)), 'out of', X_train_tfidf.shape[1]) "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set score: 0.761\n",
            "Test set score: 0.695\n",
            "Used features count: 248 out of 32943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcB4n3IP-Am4"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJcyBR5H-Am4",
        "outputId": "b6ae78e1-36b2-48f8-d5e7-be84c69d8768"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "clf = SVC(gamma='auto', kernel='linear')\n",
        "clf.fit(X_train_tfidf, y_train) \n",
        "print('Train set score: {:.3f}'.format(clf.score(X_train_tfidf, y_train))) # train data에 대한 예측정확도 \n",
        "print('Test set score: {:.3f}'.format(clf.score(X_test_tfidf, y_test))) # test data에 대한 예측정확도"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set score: 0.974\n",
            "Test set score: 0.758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjtH3SrU-Am5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}